\documentclass[conference]{IEEEtran}

%% keep it clean; only include those you need
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{bbm}
\usepackage{graphicx}
\graphicspath{{./}{../image/}}
\usepackage[colorlinks=true,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[numbers]{natbib}
\usepackage{xcolor}


%% notations; only define those you use frequently
\newcommand{\EE}{{\mathbb{E}}}
\newcommand{\PP}{{\mathbb{P}}}
\newcommand{\QQ}{{\mathbb{Q}}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\Ocal}{\mathcal{O}}


\newcommand{\sx}[1]{\textcolor{red}{(SX: #1)}}


\begin{document}


\title{\huge UNet-CRF Integration for Sequence Labeling on EEG Data}

\author{\IEEEauthorblockN{Xiaohang Ma}
\IEEEauthorblockA{\textit{Department of Mathematics} \\
\textit{University of Connecticut}\\
Storrs, CT, USA \\
xiaohang.ma@uconn.edu}
\and
\IEEEauthorblockN{Shiying Xiao}
\IEEEauthorblockA{\textit{Department of Statistics} \\
\textit{University of Connecticut}\\
Storrs, CT, USA \\
shiying.xiao@uconn.edu}
\and
\IEEEauthorblockN{Xiaohui Yin}
\IEEEauthorblockA{\textit{Department of Statistics} \\
\textit{University of Connecticut}\\
Storrs, CT, USA \\
xiaohui.yin@uconn.edu}
}

\maketitle


\begin{abstract}


%In this project, we will integrate powerful deep neural networks and
%statistical methods with a probabilistic graphical model (PGM) to effectively
%tackle sequence labeling tasks on a heterogeneous medical care dataset.


\end{abstract}


\begin{IEEEkeywords}


probabilistic graphical model, UNet, fully connected CRF,


\end{IEEEkeywords}


\section{Introduction}
%Introduction: problem definition and motivation.


Sequence labeling tasks are pivotal in diverse domains, especially healthcare,
where precise and timely data classification can significantly impact patient
outcomes. Medical datasets, such as physiological signals, often exhibit
complexity and heterogeneity, posing challenges for traditional modeling
techniques. Therefore, developing models capable of extracting meaningful
patterns from these datasets is crucial for accurate diagnostic and prognostic
decisions.


In recent years, deep learning has emerged as a powerful tool for sequence
labeling. Neural network architectures like long short-term memory (LSTM) and
Transformer models have revolutionized sequential data processing.
Additionally, conditional random fields (CRFs) have been employed for modeling
label dependencies, demonstrating promising results.
However, while these methods have shown individual strengths, a comprehensive
approach that leverages their complementary capabilities is essential for
tackling complex, multi-modal datasets.


This project introduces a novel framework that combines deep neural networks
and probabilistic graphical models (PGMs) to enhance sequence labeling
performance. By employing Transformer~\citep{vaswani2017attention} for feature
extraction and integrating them with CRF models,
we aim to improve sequence tagging accuracy. Furthermore, mean-field
approximation techniques are utilized for efficient inference, ensuring a
computationally efficient yet effective approach.
This integration of modern neural network architectures with CRFs offers
a robust and promising solution for complex sequence labeling tasks.


\section{Background}
%Background & Related Work: background info and literature survey.

\sx{Add UNet}

Deep neural networks, particularly recurrent neural networks (RNNs) like LSTM,
are widely used for sequence labeling due to their ability to capture
long-range dependencies in sequences. Bidirectional LSTM (Bi-LSTM) further
enhances this by processing sequences in both directions, improving context
understanding.
CRFs, on the other hand, are commonly used for structured predictions,
modeling dependencies between neighboring labels to ensure valid output
sequences.
\citet{huang2015bidirectional} pioneered the use of Bi-LSTM integrated with
CRFs for sequence labeling tasks, demonstrating their effectiveness in
extracting features and ensuring label consistency.


However, Bi-LSTMs can still struggle to fully account for structural
dependencies between predicted labels, leading to potential inconsistencies in
the output sequence. Additionally, they typically require large amounts of
labeled data and are computationally expensive, making them challenging to
deploy in resource-constrained environments, such as hospitals.


Transformer-based models, such as BERT~\citep{devlin2019bert},
have emerged as powerful alternatives to traditional RNN-based approaches.
Trained on massive datasets in an unsupervised manner, Transformer can be
fine-tuned for various natural language processing tasks,
including sequence labeling.
\citet{devlin2019bert} directly compared BERT to traditional models, including
Bi-LSTM, and demonstrated its superior performance on tasks like named entity
recognition, highlighting its ability to capture more feature information
than LSTM models.


While CRFs are effective for modeling label dependencies, they often lack
efficient closed-form solutions, especially for complex models beyond
linear-chain CRFs. To address this, \citet{krahenbuhl2011efficient} introduced
the use of mean-field approximation to solve more general CRFs in the context
of complex prediction tasks. By minimizing the Kullback-Leibler (KL)
divergence, mean-field approximation can be reduced to a fixed-point iteration,
providing a faster and more efficient approach for training and inference
while maintaining suitable accuracy. \citet{zheng2015conditional} further
extended this technique by reformulating mean-field approximation for fully
connected CRFs, integrating an RNN structure to capture temporal dependencies.


\section{Methods}
%Methods: overview of your proposed method, Intuition on why should it be
%better than the state of the art, and details of models and algorithms that
%you developed.


\subsection{CNN-HCRF Model}

The classical hidden conditional random field (HCRF) model can be formulated as
\begin{equation}
\begin{split}
\PP(y \vert \xb; \theta) &= \sum_{\hb \in H^N}
\mathbb{P}(y, \hb \vert \xb; \theta) \\
&= \frac{\sum_{\hb \in H^N} \exp\left(\Phi(y, \hb, \xb; \theta)\right)}
{\sum_{y^\prime} \sum_{\hb \in H^N}
\exp\left(\Phi(y^\prime, \hb, \xb; \theta)\right)}.
\end{split}
\end{equation}


Different to the design of most existing HCRF models, which they assign
potentials based on the features predefined from the input data $\Ib$.
%$\PP(y, \hb \vert \xb; \theta)$.
%Here, we consider a generative model that constructing
%$\PP(\hb \vert \xb; \theta)$ through a CRF framework then using an 
%unary potential for , 
Our HCRF replace $I$ with features learned by CNN, i.e. $\Fb(\Ib)$. 
Thus the new potentials become:
\begin{equation*}
\Phi(y, \hb, \xb; \theta) = \Phi(y, \hb, \Fb(\Ib); \theta).
%\PP(y, \hb \vert \xb; \theta) = \PP(y \vert \hb, \xb; \theta)
%\cdot \PP(\hb \vert \xb; \theta).
\end{equation*}
Thus, we design the potential of our HCRF model as.
\begin{equation}
\begin{split}
\Phi(y, \hb, \xb; \theta) &= \underbrace{
\sum_{j \in \nu} \phi(h_j, x_j; \omega)
+ \sum_{i \neq j} \psi(h_i, h_j, x_i, x_j; \eta)}_{
% \propto \log\PP(\hb \vert \xb; \theta)
\textrm{Measures log-likelihood $\log\PP(\hb \vert \xb; \theta)$}} \\
&+ \underbrace{
\sum_{j \in \nu} \varphi(y, h_j, x_j; \delta) + \vartheta(y, \xb; \varpi)}_{
% \propto \log\PP(y | \hb,  \xb; \theta)
\textrm{Measures log-likelihood $\log\PP(y \vert \hb, \xb; \theta)$}}
\end{split}
\end{equation}


{\textbf{Unary Potential}}
$\phi(h_j, x_j; \omega)$ measures the likelihood of the local feature
$x_j$ is assigned as the hidden attention state $h_j$.
We generate the likelihood through a softmax operation on $\Fb(\xb)$
the feature maps of CNN with $(N, C_{\textrm{out}})$ dimensions,
by which the potential of each pixel $x_j$ assigned as each state of
hidden state set $\{0, 1, \dots, 5\}$ is drawn and represented by
a matrix with $(N, 2)$ dimension.


The parameter $\omega$ is learned by our end-to-end CNN-Unet training 
structure.


{\textbf{Unary Potential}}
$\varphi(y, h_j; \delta)$ measures the compatibility between global
class label $y$ and the hidden local state $h_j$. This potential is
parametrized as:
\begin{equation*}
\varphi(y, h_{j}; \delta) = \sum_{a \in Y} \sum_{b \in H} \delta_{a, b}
\cdot \one(y = a) \cdot \one(h_j = b).
\end{equation*}


{\textbf{Binary Potential}}
$\psi(h_i, h_j, x_i, x_j; \eta)$
balances the hidden label compatibility of the neighboring pixels on the 
feature image $\Fb(\Ib)$.
We define this potential through a common contrast-sensitive two-kernal 
potentials~\citep{krahenbuhl2011efficient, chen2022end}:
\begin{equation}
\begin{split}
& \psi(h_i, h_j, x_i, x_j; \eta) = \mu(h_i, h_j) \Bigg[
\omega_1 \exp \bigg(
-\frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\alpha^2} \\
&\quad
- \frac{\left\lvert \Fb_i(\Ib) - \Fb_j(\Ib)\right\rvert^2}{2\eta_\beta^2}
\bigg)
+ \omega_2 \exp \left(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2 \eta_\gamma^2}
\right)
\Bigg]
\end{split}
\end{equation}


\subsection{Variational Inference of HCRF}


\subsubsection{ELBO Under Mean-Filed Variational Family}


The existence of the hidden variable $\hb$ makes our model more interpretable.
However, since We will train our model through maximal likelihood method,
introducing the hidden variable will make precisely computation the summation
$\sum_{\hb \in H^N} \PP(y, \hb \vert \xb; \theta)$ intractable.
To address this issue, we make our computation more feasible through a
mean-field variational inference framework, which enables our model
efficiently estimating the log-likelihood $\log\PP(y \vert \xb; \theta)$.


Firstly, we introduce the mean-field varepsilon family approximation of
$\log\PP(\hb \vert y, \xb; \theta)$ as:
\begin{equation*}
\QQ(\hb) = \prod_{i=1}^N q_i(h_i).
\end{equation*}
Then the evidence lower bound (ELBO) under the mean-field family is given by
\begin{equation}
\begin{split}
& \textrm{ELBO}(\QQ) = \EE_{\QQ(\hb)} \log\PP(y, \hb \vert \xb; \theta)
- \EE_{\QQ(\hb)} \log q(\hb) \\
&\quad = \sum_{i=1}^N \EE_{q_i(h_i)}
\left[ \phi(h_j, x_j; \omega) + \varphi(y, h_j, x_j; \delta) \right] \\
&\quad + \sum_{i=1}^N \sum_{j=1}^N \sum_{l,l^\prime} \one_{\{i \neq j\}} 
\QQ(h_i = l) \QQ(h_j = l^\prime) \mu(l, l^\prime) \\
&\quad \Bigg[ \omega_1 \exp \left(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\alpha^2}
- \frac{\left\lvert \Fb_i(\Ib) - \Fb_j(\Ib) \right\rvert^2}{2\eta_\beta^2}
\right) \\
&\quad + \omega_2 \exp \left(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\gamma^2}
\right) \Bigg] \\
&\quad - \sum_{i=1}^N \EE_{q_i} \log q_i(h_i) + \vartheta(y, \xb; \varpi).
\end{split}
\end{equation}


\subsubsection{Updating Variational Family}


By the coordinate ascent variational inference(CAVI) algorithm,
we can obtain that fix $q_j(h_j), \forall j \neq i$, the optimal $q_i(h_i)$
that maximizes $\textrm{ELBO}(q_i(h_i))$ is given by
\begin{equation}
\QQ(h_i) \propto \exp\{\EE_{\QQ(\hb_{-i})}
\log\PP(y, h_i, \hb_{-i} \vert \xb; \theta)\}.
\end{equation}
Thus, we can get that
\begin{equation}
\begin{split}
q_i(l) &= \frac{1}{Z_i} \exp \Bigg\{
\phi(h_j, x_j; \omega) + \varphi(y, h_j, x_j; \delta) \\
&+ \sum_{j \neq i} \sum_{l^\prime} q(l^\prime) \mu(l, l^\prime) 
\Bigg[ \omega_1 \exp \big(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\alpha^2}\\
&- \frac{\left\lvert \Fb_i(\Ib) - \Fb_j(\Ib) \right\rvert^2}{2\eta_\beta^2}
\big)
+ \omega_2 \exp \big(
- \frac{\left\lvert p_i - p_j 
\right\rvert^2}{2\eta_\gamma^2}
\big)
\Bigg]
\Bigg\}
\end{split}
\end{equation}


\subsubsection{Computational Complexity Analysis}


\begin{itemize}
\item {\textbf{Updating Variational Family}} $\{q_i(h_i)\}$:
The computationally expensive part of the fixed point iteration of $q_i(h_i)$
comes from the message passing, i.e. the convolution of $\QQ(\hb)$ and
Gaussian kernal in $\psi(h_i, h_j, x_i, x_j; \eta)$. We need $\Ocal(N^2)$
runtime for $N$ pixels when evaluating this term precisely.
To make the computation more feasible, a truncated gaussian kernal will be
employed, which is only supported on spaces within certain proportion of
the untruncated standard deviation. Then the approximate message passing is
linear in the number of pixels $N$.
\item {\textbf{Computation of the ELBO}}: The summation over the product of
$q_i(h_i)q_j(h_j)$ and the gaussian kernal in $\psi(h_i, h_j, x_i, x_j; \eta)$
also requires a $\Ocal(N^2)$ complexity in time.
Owes to a similar truncated gaussian kernal approxiamtion, the runtime can
be reduced to $\Ocal(N)$.
\end{itemize}


\section{Results}
%Results: Description of your experiments and results and a list of questions
%your experiments are designed to answer.


\section{Conclusion}
%Conclusion: discussion and future work.



\bibliographystyle{IEEEtranN}
\bibliography{refs}

\end{document}
