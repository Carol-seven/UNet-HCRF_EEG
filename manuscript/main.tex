\documentclass[conference]{IEEEtran}

%% keep it clean; only include those you need
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{bbm}
\usepackage{graphicx}
\graphicspath{{./}{../image/}}
\usepackage[colorlinks=true,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[numbers]{natbib}
\usepackage{xcolor}


%% notations; only define those you use frequently
\newcommand{\EE}{{\mathbb{E}}}
\newcommand{\Fb}{\mathbf{F}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\PP}{{\mathbb{P}}}
\newcommand{\QQ}{{\mathbb{Q}}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\RR}{{\mathbb{R}}}

\let\oldsubsubsection\subsubsection
\renewcommand{\subsubsection}[1]{\oldsubsubsection{\textbf{#1}}}


\newcommand{\sx}[1]{\textcolor{red}{(SX: #1)}}


\begin{document}


\title{\huge UNet-HCRF Integration for EEG Pattern Recognition}

\author{\IEEEauthorblockN{Xiaohang Ma}
\IEEEauthorblockA{\textit{Department of Mathematics} \\
\textit{University of Connecticut}\\
Storrs, CT, USA \\
xiaohang.ma@uconn.edu}
\and
\IEEEauthorblockN{Shiying Xiao}
\IEEEauthorblockA{\textit{Department of Statistics} \\
\textit{University of Connecticut}\\
Storrs, CT, USA \\
shiying.xiao@uconn.edu}
\and
\IEEEauthorblockN{Xiaohui Yin}
\IEEEauthorblockA{\textit{Department of Statistics} \\
\textit{University of Connecticut}\\
Storrs, CT, USA \\
xiaohui.yin@uconn.edu}
}

\maketitle


\begin{abstract}


%In this project, we will integrate powerful deep neural networks and
%statistical methods with a probabilistic graphical model (PGM) to effectively
%tackle sequence labeling tasks on a heterogeneous medical care dataset.


\end{abstract}


\begin{IEEEkeywords}


classification, probabilistic graphical model, spectrogram,
variational inference, wavelet transformation


\end{IEEEkeywords}


\section{Introduction}
%Introduction: problem definition and motivation.


\sx{EEG tasks}

Sequence labeling tasks are pivotal in diverse domains, especially healthcare,
where precise and timely data classification can significantly impact patient
outcomes. Medical datasets, such as physiological signals, often exhibit
complexity and heterogeneity, posing challenges for traditional modeling
techniques. Therefore, developing models capable of extracting meaningful
patterns from these datasets is crucial for accurate diagnostic and prognostic
decisions.


In recent years, deep learning has emerged as a powerful tool for sequence
labeling. Neural network architectures like long short-term memory (LSTM) and
Transformer models have revolutionized sequential data processing.
Additionally, conditional random fields (CRFs) have been employed for modeling
label dependencies, demonstrating promising results.
However, while these methods have shown individual strengths, a comprehensive
approach that leverages their complementary capabilities is essential for
tackling complex, multi-modal datasets.


This project introduces a novel framework that combines deep neural networks
and probabilistic graphical models (PGMs) to enhance sequence labeling
performance. By employing Transformer~\citep{vaswani2017attention} for feature
extraction and integrating them with CRF models,
we aim to improve sequence tagging accuracy. Furthermore, mean-field
approximation techniques are utilized for efficient inference, ensuring a
computationally efficient yet effective approach.
This integration of modern neural network architectures with CRFs offers
a robust and promising solution for complex sequence labeling tasks.


\section{Background}
%Background & Related Work: background info and literature survey.

\sx{Add UNet}

Deep neural networks, particularly recurrent neural networks (RNNs) like LSTM,
are widely used for sequence labeling due to their ability to capture
long-range dependencies in sequences. Bidirectional LSTM (Bi-LSTM) further
enhances this by processing sequences in both directions, improving context
understanding.
CRFs, on the other hand, are commonly used for structured predictions,
modeling dependencies between neighboring labels to ensure valid output
sequences.
\citet{huang2015bidirectional} pioneered the use of Bi-LSTM integrated with
CRFs for sequence labeling tasks, demonstrating their effectiveness in
extracting features and ensuring label consistency.


However, Bi-LSTMs can still struggle to fully account for structural
dependencies between predicted labels, leading to potential inconsistencies in
the output sequence. Additionally, they typically require large amounts of
labeled data and are computationally expensive, making them challenging to
deploy in resource-constrained environments, such as hospitals.


Transformer-based models, such as BERT~\citep{devlin2019bert},
have emerged as powerful alternatives to traditional RNN-based approaches.
Trained on massive datasets in an unsupervised manner, Transformer can be
fine-tuned for various natural language processing tasks,
including sequence labeling.
\citet{devlin2019bert} directly compared BERT to traditional models, including
Bi-LSTM, and demonstrated its superior performance on tasks like named entity
recognition, highlighting its ability to capture more feature information
than LSTM models.


While CRFs are effective for modeling label dependencies, they often lack
efficient closed-form solutions, especially for complex models beyond
linear-chain CRFs. To address this, \citet{krahenbuhl2011efficient} introduced
the use of mean-field approximation to solve more general CRFs in the context
of complex prediction tasks. By minimizing the Kullback-Leibler (KL)
divergence, mean-field approximation can be reduced to a fixed-point iteration,
providing a faster and more efficient approach for training and inference
while maintaining suitable accuracy. \citet{zheng2015conditional} further
extended this technique by reformulating mean-field approximation for fully
connected CRFs, integrating an RNN structure to capture temporal dependencies.


\section{Methods}


We begin by introducing the necessary notations.
Let $\xb = (x_0, x_1, \dots, x_N)^\top$ represent the feature vector of the
input image, where $x_0$ denotes the global feature capturing overall image
characteristics, and $x_i$ ($1 \leq i \leq N$) corresponds to a local feature
extracted from the $i$-th pixel. Here, $N$ denotes the total number of pixels
in the image. Let $y \in Y$ represent the class label associated with the image,
where $Y$ is a finite set containing $\vert Y \vert$ possible class labels.
For any feature $\xb$, a corresponding vector of unobserved variables, referred
to as hidden part states and denoted by $\hb = (h_1, \dots, h_N)^\top$,
is introduced. Each $h_i \in H$ represents the hidden part state associated
with $x_i$ for $i = 1, \dots, N$. Here, $H$ is a finite set of hidden part
states, and $\vert H \vert$ denotes the cardinality of this set.


\subsection{CNN-HCRF Model}


Following the theory of random fields as outlined
by~\citet{quattoni2004conditional} and~\citet{wang2006hidden},
given the feature $\xb$, its corresponding hidden part states $\hb$,
and the class label $y$, a hidden conditional random field (HCRF) can be
expressed in exponential form as follows:
\begin{equation}
\label{eq:hcrf}
\PP(y, \hb \mid \xb; \theta)
= \frac{\exp\left(\Phi(y, \hb, \xb; \theta)\right)}
{\sum_{y^\prime \in Y} \sum_{\hb \in H^N}
\exp\left(\Phi(y^\prime, \hb, \xb; \theta)\right)},
\end{equation}
where $\theta$ is the parameter of the model,
$H^N$ denotes the set of all possible hidden part states of $N$ hidden parts,
and $\Phi(y, \hb, \xb; \theta) \in \RR$ refers to a potential function
depending on the feature $\xb$ and parameterized by $\theta$.


From Equation~\ref{eq:hcrf}, the probability of class label $y$ for the given
feature $\xb$ is:
\begin{equation*}
\begin{split}
\PP(y \vert \xb; \theta) &= \sum_{\hb \in H^N} \PP(y, \hb \mid \xb; \theta) \\
&= \frac{\sum_{\hb \in H^N} \exp\left(\Phi(y, \hb, \xb; \theta)\right)}
{\sum_{y^\prime \in Y} \sum_{\hb \in H^N}
\exp\left(\Phi(y^\prime, \hb, \xb; \theta)\right)}.
\end{split}
\end{equation*}


The potential of the HCRF model, $\Phi(y, \hb, \xb; \theta)$, is defined in the
following form:
\begin{align*}
\Phi(y, \hb, \xb; \theta) &= \underbrace{
\sum_{j \in \nu} \phi(h_j, x_j; \omega) +
\sum_{i \neq j} \psi(h_i, h_j, x_i, x_j; \eta)}_{
% \propto \log \PP( \hb \vert \xb; \theta)
\textrm{Measures log-likelihood $\log \PP(\hb \vert \xb; \theta)$}} \\
& + \underbrace{
\sum_{j \in \nu} \varphi(y, h_j, x_j; \delta) + \vartheta(y, x_0; \varpi)}_{
% \propto \log \PP(y | \hb, \xb; \theta)
\textrm{Measures log-likelihood $\log \PP(y \vert \hb, \xb; \theta)$}},
\end{align*}
where the detailed components are described below.


\textbf{Unary potential} $\phi(h_j, x_j; \omega)$ measures the likelihood of
the local feature $x_j$ is assigned as the hidden attention state $h_j$.
The parameter $\omega$ is learned through the end-to-end CNN-UNet training
structure. This likelihood is computed by applying a softmax operation to the
feature maps produced by the CNN.


\textbf{Binary potential} $\psi(h_i, h_j, x_i, x_j; \eta)$ balances the
hidden label compatibility of the neighboring pixels on the feature $\xb$.
The potential is defined through a common contrast-sensitive two-kernal
potentials~\citep{krahenbuhl2011efficient, chen2022end}:
\begin{equation*}
\begin{split}
& \psi(h_i, h_j, x_i, x_j; \eta) = \mu(h_i, h_j) \Bigg[
\omega_1 \exp \bigg(
-\frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\alpha^2} \\
&\quad
- \frac{\left\lvert x_i - x_j\right\rvert^2}{2\eta_\beta^2}
\bigg) + \omega_2 \exp \left(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2 \eta_\gamma^2}
\right)
\Bigg],
\end{split}
\end{equation*}
where $\mu(h_i, h_j)$ is a label compatibility function that describes
the interactive influences between different pairs of classes,
and $p$ refers to the pixel position.
The parameters $\omega_1$ and $\omega_2$ serve as linear combination weights,
controlling the contributions of the first and second kernels, respectively.
And the parameters $\eta_\alpha$, $\eta_\beta$ and $\eta_\gamma$ control the
influence of the corresponding feature spaces.


\textbf{Unary potential} $\varphi(y, h_j; \delta)$ measures the compatibility
between global class label $y$ and the hidden local state $h_j$.
This potential is parametrized as:
\begin{equation*}
\varphi(y, h_{j}; \delta) = \sum_{a \in Y} \sum_{b \in H} \delta_{a, b}
\cdot \one(y = a) \cdot \one(h_j = b).
\end{equation*}
where $\one(\cdot)$ is an indicator function, and $\delta_{a,b}$ denotes the
likelihood of class label $y = a$ containing a joint with hidden state
$h_j = b$, which is learned during training.


\textbf{Global potential} $\vartheta(y, x_0; \varpi)$ measures the likelihood of
the global feature $x_0$ is assigned as the class label $y$, where the parameter
$\varpi$ is learned during training process.


\subsection{Variational Inference of HCRF}


The inclusion of the hidden variable $\hb$ enhances the interpretability of the
model. However, training the model using maximum likelihood estimation
necessitates the calculation of the summation
$\sum_{\hb \in H^N} \PP(y, \hb \mid \xb; \theta)$, and the presence of the
hidden variable renders the exact computation of this summation intractable.
To circumvent this challenge, a mean-field variational inference framework is
employed, enabling efficient estimation of the log-likelihood
$\log \PP(y \mid \xb; \theta)$.


\subsubsection{ELBO Under Mean-Filed Variational Family}


The mean-field varepsilon family approximation of
$\log \PP(\hb \mid y, \xb; \theta)$ is given by:
\begin{equation*}
\QQ(\hb) = \prod_{i=1}^N q_i(h_i),
\end{equation*}
where $q_i(h_i)$ represents the approximate distribution of the individual
hidden variable.


Under the mean-field family, the evidence lower bound (ELBO) is given by
\begin{equation}
\begin{split}
& \Lc(\QQ) = \EE_{\QQ(\hb)} \log\PP(y, \hb \mid \xb; \theta)
- \EE_{\QQ(\hb)} \log q(\hb) \\
&\quad = \sum_{i=1}^N \EE_{q_i(h_i)}
\left[ \phi(h_j, x_j; \omega) + \varphi(y, h_j, x_j; \delta) \right] \\
&\quad + \sum_{i=1}^N \sum_{j=1}^N \sum_{l,l^\prime} \one_{\{i \neq j\}} 
\QQ(h_i = l) \QQ(h_j = l^\prime) \mu(l, l^\prime) \\
&\quad \Bigg[ \omega_1 \exp \left(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\alpha^2}
- \frac{\left\lvert x_i - x_j \right\rvert^2}{2\eta_\beta^2} \right) \\
&\quad + \omega_2 \exp \left(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\gamma^2}
\right) \Bigg] \\
&\quad - \sum_{i=1}^N \EE_{q_i} \log q_i(h_i) + \vartheta(y, x_0; \varpi).
\end{split}
\end{equation}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\textwidth]{workframe}
\caption{\sx{caption}.}
\label{fig:workframe}
\end{figure*}


\subsubsection{Updating Variational Family}


By employing the coordinate ascent variational inference (CAVI) algorithm,
an efficient method for approximating posterior distributions in probabilistic
models, the optimization process involves iteratively updating each variational
factor while holding others fixed. Specifically, fixing $q_j(h_j)$,
$\forall j \neq i$, the optimal $q_i(h_i)$ that maximizes $\Lc(q_i(h_i))$
is computed as:
\begin{equation*}
\QQ(h_i) \propto \exp\{\EE_{\QQ(\hb_{-i})}
\log\PP(y, h_i, \hb_{-i} \mid \xb; \theta)\}.
\end{equation*}
Thus, we can get that
\begin{equation*}
\begin{split}
q_i(l) &= \frac{1}{Z_i} \exp \Bigg\{
\phi(h_j, x_j; \omega) + \varphi(y, h_j, x_j; \delta) \\
&+ \sum_{j \neq i} \sum_{l^\prime} q(l^\prime) \mu(l, l^\prime) 
\Bigg[ \omega_1 \exp \bigg(
- \frac{\left\lvert p_i - p_j \right\rvert^2}{2\eta_\alpha^2}\\
&- \frac{\left\lvert x_i - x_j \right\rvert^2}{2\eta_\beta^2}
\bigg)
+ \omega_2 \exp \left(
- \frac{\left\lvert p_i - p_j 
\right\rvert^2}{2\eta_\gamma^2}
\right)
\Bigg]
\Bigg\},
\end{split}
\end{equation*}
where $Z_i$ is the normalizing constant.


\subsubsection{Computational Complexity Analysis}


\paragraph{Updating variational family}


The computationally intensive step in the fixed-point iteration of $q_i(h_i)$
is the message passing process, which involves convolving $\QQ(\hb)$ with a
Gaussian kernel within the binary potential $\psi(h_i, h_j, x_i, x_j; \eta)$.
This operation requires $\Oc(N^2)$ runtime for $N$ pixels when evaluated
exactly. To improve computational efficiency, a truncated Gaussian kernel is
utilized, which is supported only within a specific proportion of the
untruncated standard deviation. This approximation reduces the complexity of 
message passing to linear time, $\Oc(N)$.


\paragraph{Computation of the ELBO}


The summation over the product of $q_i(h_i)q_j(h_j)$ and the Gaussian kernel
in $\psi(h_i, h_j, x_i, x_j; \eta)$ also incurs a time complexity of $\Oc(N^2)$.
By applying a similar truncated Gaussian kernel approximation, the runtime can
be reduced to $\Oc(N)$.


\section{Results}


\section{Conclusion}


\bibliographystyle{IEEEtranN}
\bibliography{refs}

\end{document}
